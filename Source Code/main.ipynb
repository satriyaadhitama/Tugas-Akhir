{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision, Accuracy, categorical_accuracy\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load Audio List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = {\n",
    "    \"words\": os.path.join(\"data\", \"words\"),\n",
    "    \"list\": os.path.join(\"data\", \"list\"),\n",
    "    \"background_noise\": os.path.join(\"data\", \"_background_noise_\"),\n",
    "    \"presentation\": os.path.join(\"data\", \"presentation\")\n",
    "}\n",
    "\n",
    "# word_list = os.listdir(file_path[\"words\"])\n",
    "word_list = [\"forward\",\"backward\",\"stop\",\"go\",\"on\",\"of\"]\n",
    "words_exception = ['bed', 'bird', 'cat', 'dog', 'happy', 'house', 'learn', 'left', 'marvin', 'sheila', 'visual', 'wow']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLITTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.listdir(file_path[\"list\"])\n",
    "list_dict = {}\n",
    "for _list in data_list:\n",
    "    list_dict[_list.replace(\".txt\",\"\")] = list(map(lambda x : x.replace(\"\\n\",\"\"), open(os.path.join(file_path[\"list\"], _list)).readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['testing_list', 'training_list', 'validation_list'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keseluruhan Data Training : 84843\n",
      "Keseluruhan Data Testing: 11005\n",
      "Keseluruhan Data Validation : 9981\n"
     ]
    }
   ],
   "source": [
    "print(\"Keseluruhan Data Training :\", len(list_dict[\"training_list\"]))\n",
    "print(\"Keseluruhan Data Testing:\", len(list_dict[\"testing_list\"]))\n",
    "print(\"Keseluruhan Data Validation :\", len(list_dict[\"validation_list\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Limited Number of Training, Testing, and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_word_file_list(word_file_list:list, n:int):\n",
    "    new_list = []\n",
    "    word_count = 0\n",
    "    curr_word = \"\"  \n",
    "    for i in word_file_list:\n",
    "        word, file = i.split('/')\n",
    "        if word_count < n:\n",
    "            new_list.append(i)\n",
    "        if curr_word != word and curr_word != \"\": \n",
    "            new_list.append(i)\n",
    "            word_count = 0\n",
    "        curr_word = word\n",
    "        word_count += 1\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list_dict:\n",
    "    _list = list_dict[key]\n",
    "    res = []\n",
    "    for word in word_list:\n",
    "        res.extend(list(filter(lambda x: word in x, _list)))\n",
    "    list_dict[key] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = limited_word_file_list(list_dict[\"training_list\"], 1200)\n",
    "testing_files = limited_word_file_list(list_dict[\"testing_list\"], 200)\n",
    "validation_files = limited_word_file_list(list_dict[\"validation_list\"], 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_files = [testing_files, training_files, validation_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satri\\AppData\\Local\\Temp\\ipykernel_5616\\2271496847.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list = df_list.append(curr_df)\n",
      "C:\\Users\\satri\\AppData\\Local\\Temp\\ipykernel_5616\\2271496847.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list = df_list.append(curr_df)\n",
      "C:\\Users\\satri\\AppData\\Local\\Temp\\ipykernel_5616\\2271496847.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list = df_list.append(curr_df)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>word</th>\n",
       "      <th>file</th>\n",
       "      <th>usage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forward/bb05582b_nohash_3.wav</td>\n",
       "      <td>forward</td>\n",
       "      <td>bb05582b_nohash_3.wav</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forward/f2e59fea_nohash_3.wav</td>\n",
       "      <td>forward</td>\n",
       "      <td>f2e59fea_nohash_3.wav</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forward/c9b5ff26_nohash_4.wav</td>\n",
       "      <td>forward</td>\n",
       "      <td>c9b5ff26_nohash_4.wav</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forward/837a0f64_nohash_1.wav</td>\n",
       "      <td>forward</td>\n",
       "      <td>837a0f64_nohash_1.wav</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forward/e49428d9_nohash_0.wav</td>\n",
       "      <td>forward</td>\n",
       "      <td>e49428d9_nohash_0.wav</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>off/ccea893d_nohash_1.wav</td>\n",
       "      <td>off</td>\n",
       "      <td>ccea893d_nohash_1.wav</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>off/c4cfbe43_nohash_1.wav</td>\n",
       "      <td>off</td>\n",
       "      <td>c4cfbe43_nohash_1.wav</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>off/0ea9c8ce_nohash_3.wav</td>\n",
       "      <td>off</td>\n",
       "      <td>0ea9c8ce_nohash_3.wav</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>off/4e6902d0_nohash_0.wav</td>\n",
       "      <td>off</td>\n",
       "      <td>4e6902d0_nohash_0.wav</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>off/c6ee87a7_nohash_2.wav</td>\n",
       "      <td>off</td>\n",
       "      <td>c6ee87a7_nohash_2.wav</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11023 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path     word                   file  \\\n",
       "0     forward/bb05582b_nohash_3.wav  forward  bb05582b_nohash_3.wav   \n",
       "1     forward/f2e59fea_nohash_3.wav  forward  f2e59fea_nohash_3.wav   \n",
       "2     forward/c9b5ff26_nohash_4.wav  forward  c9b5ff26_nohash_4.wav   \n",
       "3     forward/837a0f64_nohash_1.wav  forward  837a0f64_nohash_1.wav   \n",
       "4     forward/e49428d9_nohash_0.wav  forward  e49428d9_nohash_0.wav   \n",
       "...                             ...      ...                    ...   \n",
       "1296      off/ccea893d_nohash_1.wav      off  ccea893d_nohash_1.wav   \n",
       "1297      off/c4cfbe43_nohash_1.wav      off  c4cfbe43_nohash_1.wav   \n",
       "1298      off/0ea9c8ce_nohash_3.wav      off  0ea9c8ce_nohash_3.wav   \n",
       "1299      off/4e6902d0_nohash_0.wav      off  4e6902d0_nohash_0.wav   \n",
       "1300      off/c6ee87a7_nohash_2.wav      off  c6ee87a7_nohash_2.wav   \n",
       "\n",
       "           usage  label  \n",
       "0        testing      1  \n",
       "1        testing      1  \n",
       "2        testing      1  \n",
       "3        testing      1  \n",
       "4        testing      1  \n",
       "...          ...    ...  \n",
       "1296  validation      3  \n",
       "1297  validation      3  \n",
       "1298  validation      3  \n",
       "1299  validation      3  \n",
       "1300  validation      3  \n",
       "\n",
       "[11023 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_list = pd.DataFrame()\n",
    "\n",
    "for i in range(len(list_dict.keys())):\n",
    "    curr_df = pd.DataFrame(limited_files[i])\n",
    "    curr_df = curr_df.join(pd.Series(limited_files[i]).str.split(\"/\",expand=True), lsuffix='_caller', rsuffix='_other')\n",
    "    curr_df.rename(columns={\"0_caller\":\"path\"},inplace=True)\n",
    "    curr_df.rename(columns={\"0_other\":\"word\"},inplace=True)\n",
    "    curr_df.rename(columns={1:\"file\"},inplace=True)\n",
    "    curr_df[\"usage\"] = list(list_dict.keys())[i].replace(\"_list\",\"\")\n",
    "    # Label word\n",
    "    curr_df[\"label\"] = curr_df[\"word\"]\n",
    "    curr_df[\"label\"] = label_encoder.fit_transform(curr_df[\"label\"])\n",
    "    \n",
    "    df_list = df_list.append(curr_df)\n",
    "    \n",
    "df_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_list[df_list['word'] != 'one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training      7200\n",
       "testing       1122\n",
       "validation    1101\n",
       "Name: usage, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"usage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>usage</th>\n",
       "      <th>testing</th>\n",
       "      <th>training</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>backward</th>\n",
       "      <td>166</td>\n",
       "      <td>1200</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward</th>\n",
       "      <td>155</td>\n",
       "      <td>1200</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>200</td>\n",
       "      <td>1200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>200</td>\n",
       "      <td>1200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>200</td>\n",
       "      <td>1200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>201</td>\n",
       "      <td>1200</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "usage     testing  training  validation\n",
       "word                                   \n",
       "backward      166      1200         154\n",
       "forward       155      1200         146\n",
       "go            200      1200         200\n",
       "off           200      1200         200\n",
       "on            200      1200         200\n",
       "stop          201      1200         201"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df, columns=\"usage\",index=\"word\",values=\"file\",aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satri\\AppData\\Local\\Temp\\ipykernel_5616\\1238965708.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"path\"] = df[\"path\"].map(define_file_path)\n"
     ]
    }
   ],
   "source": [
    "# Define file path in the dataframe\n",
    "def define_file_path(x):\n",
    "    word, file_name = x.split(\"/\")\n",
    "    return os.path.join(\"data\",\"words\",word,file_name)\n",
    "\n",
    "df[\"path\"] = df[\"path\"].map(define_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load WAV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(filename):\n",
    "    try :\n",
    "        # Load encoded wav file\n",
    "        file_contents = tf.io.read_file(filename)\n",
    "        # Decode wav (tensors by channels) \n",
    "        wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "        # Removes trailing axis\n",
    "        wav = tf.squeeze(wav, axis=-1)\n",
    "        return wav\n",
    "    except : pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convert to Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path, label): \n",
    "    wav = load_wav(file_path)\n",
    "    wav = wav[:16000]\n",
    "    zero_padding = tf.zeros([16000] - tf.shape(wav), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, wav],0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create DAta Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df[(df[\"usage\"] == \"training\")].copy()\n",
    "df_testing = df[(df[\"usage\"] == \"testing\")].copy()\n",
    "df_validation = df[(df[\"usage\"] == \"validation\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = LabelBinarizer().fit_transform(df_training[\"word\"])\n",
    "y_test = LabelBinarizer().fit_transform(df_testing[\"word\"])\n",
    "y_validation = LabelBinarizer().fit_transform(df_validation[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_training = tf.data.Dataset.from_tensor_slices((df_training[\"path\"].values, y_train))\n",
    "tf_testing = tf.data.Dataset.from_tensor_slices((df_testing[\"path\"].values, y_test))\n",
    "tf_validation = tf.data.Dataset.from_tensor_slices((df_validation[\"path\"].values, y_validation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tensorflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 2000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_training = tf_training.map(preprocess)\n",
    "tf_training = tf_training.shuffle(SHUFFLE_BUFFER)\n",
    "tf_training = tf_training.cache()\n",
    "tf_training = tf_training.batch(BATCH_SIZE, drop_remainder=True)\n",
    "tf_training = tf_training.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_testing = tf_testing.map(preprocess)\n",
    "tf_testing = tf_testing.shuffle(SHUFFLE_BUFFER)\n",
    "tf_testing = tf_testing.cache()\n",
    "tf_testing = tf_testing.batch(BATCH_SIZE, drop_remainder=True)\n",
    "tf_testing = tf_testing.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_validation = tf_validation.map(preprocess)\n",
    "tf_validation = tf_validation.shuffle(SHUFFLE_BUFFER)\n",
    "tf_validation = tf_validation.cache()\n",
    "tf_validation = tf_validation.batch(BATCH_SIZE, drop_remainder=True)\n",
    "tf_validation = tf_validation.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = tf_training.as_numpy_iterator().next()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (491, 257)\n"
     ]
    }
   ],
   "source": [
    "input_shape = samples.shape[1:3]\n",
    "print(\"input shape:\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "model.add(layers.LSTM(64, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 491, 128)          197632    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 491, 64)           49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 491, 64)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 31424)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                2011200   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,258,630\n",
      "Trainable params: 2,258,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=CategoricalCrossentropy(), \n",
    "              metrics=[Recall(),Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 4\n",
    "\n",
    "# hist = model.fit(\n",
    "#     tf_training, \n",
    "#     epochs=EPOCHS, \n",
    "#     validation_data=tf_validation, \n",
    "#     callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.saved_model.save(model, \"LSTM_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_NAME = \"LSTM_1\"\n",
    "LOAD_MODEL_PATH = os.path.join(\"model\",LOAD_MODEL_NAME)\n",
    "loaded_model = tf.saved_model.load(LOAD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64,6] vs. [64,491,257,6] [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m input_tensor_y\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(y)\n\u001b[0;32m     10\u001b[0m input_tensor_y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(input_tensor_y, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 11\u001b[0m input_tensor_y \u001b[39m=\u001b[39m input_tensor_y \u001b[39m*\u001b[39;49m tf\u001b[39m.\u001b[39;49mones([\u001b[39m64\u001b[39;49m, \u001b[39m491\u001b[39;49m, \u001b[39m257\u001b[39;49m, \u001b[39m6\u001b[39;49m])\n\u001b[0;32m     13\u001b[0m \u001b[39m# Concatenate input tensors along the desired axis\u001b[39;00m\n\u001b[0;32m     14\u001b[0m input_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([input_tensor_X, input_tensor_y], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\satri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\satri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64,6] vs. [64,491,257,6] [Op:Mul]"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "for tf_data in tf_testing:\n",
    "    X, y = tf_data\n",
    "    # Assuming 'lstm_input1' and 'lstm_input2' are the expected input names in the signature\n",
    "    # Prepare input data for prediction\n",
    "    input_tensor_X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    \n",
    "    input_tensor_y= tf.convert_to_tensor(y)\n",
    "    input_tensor_y = tf.cast(input_tensor_y, dtype=tf.float32)\n",
    "    input_tensor_y = input_tensor_y * tf.ones([64, 491, 257, 6])\n",
    "    \n",
    "    # Concatenate input tensors along the desired axis\n",
    "    input_tensor = tf.concat([input_tensor_X, input_tensor_y], axis=0)\n",
    "    \n",
    "    ypred = loaded_model.signatures['serving_default'](lstm_input=input_tensor)\n",
    "    y_pred = np.argmax(ypred, axis=1)\n",
    "    y_test = np.argmax(y, axis=1)\n",
    "    \n",
    "    predicted.append(y_pred)\n",
    "    actual.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array(actual).flatten()\n",
    "predicted = np.array(predicted).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actual, predicted)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=word_list)\n",
    "\n",
    "cm_display.plot()\n",
    "plt.title(\"Confusion Matrix \")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84c8bcfd3ece7b8d2e9484bf4ce8517a5a4dbcc142a29f247d8a18b531027092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
